---
title: "Muskox Collar Data Cleaning"
format: html
editor: visual
execute: 
  cache: TRUE
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library(sf)
library(here)
```

```{r}
#| label: load-data
#| include: false

musk_collar <- readRDS(here("data/processed/musk_collar.rds"))
```

## Sample Size

Movement data for this project come from `r length(unique(musk_collar$Id_Number))` female muskoxen in the Sahtu region of the Northwest Territories (NWT) that were collared between `r format(min(musk_collar$Date), "%B %Y")` and `r format(max(musk_collar$Date), "%B %Y")`.

We can plot the date ranges of individual collared muskoxen below:

```{r}
#| label: daterange
#| dependson: "load-data"
#| echo: FALSE

daterange <- musk_collar %>% 
  sf::st_drop_geometry() %>% 
  group_by(Id_Number) %>% 
  summarize(DateRange = paste(format(min(Date), "%b %Y"), 
                              format(max(Date), "%b %Y"), sep = " to "),
            NumDays = as.numeric(difftime(max(Date), min(Date), 
                                            units = "days")))

knitr::kable(daterange) %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

Muskoxen `r head(daterange$Id_Number[which(daterange$NumDays<365)],-1)`, and `r tail(daterange$Id_Number[which(daterange$NumDays<365)],1)` have less than a year of movement data and only `r head(daterange$Id_Number[which(daterange$NumDays>365*2)],-1)`, and `r tail(daterange$Id_Number[which(daterange$NumDays>365*2)],1)` have more than 2 years of data.

## Collar Failure

Before we look into the data, there are some data cleaning steps we can use to remove errors from that dataset. First, we know that collars 707, 709, and 710 had their release mechanisms triggered early, so we need to look at the data to figure out when this happened. Let's start by looking at histograms of step lengths:

```{r}
#| label: dist_hist
#| dependson: "load-data"
#| echo: FALSE
#| warning: FALSE

musk_collar %>%
  ## add one to distances to allow 0s to be plotted on log scale
  mutate(steplength_m = steplength_m + 1) %>%
  ggplot(aes(x = steplength_m)) +
  geom_histogram(fill = "darkblue") +
  scale_x_log10() +
  facet_wrap(~Id_Number, scales = "free_y") +
  theme_bw()
```

The stepped nature of the distances traveled on the left side of the graph are a result of the resolution of the GPS device. For collars 707, 709, and 710, there appear to be a greater-than-expected number of instances where the muskoxen move negligible distances. By plotting time series of step lengths, we can can see that these negligible distances occur at the end of the recording periods:

```{r, fig.height=6, fig.width=8}
#| label: dist_time
#| dependson: "load-data"
#| echo: FALSE
#| warning: FALSE

musk_collar %>%
  ## add one to distances to allow 0s to be plotted on log scale
  mutate(steplength_m = steplength_m + 1) %>%
  ggplot(aes(x = datetime, y = steplength_m)) +
  geom_point(size = 0.5) +
  scale_y_log10() +
  facet_wrap(~Id_Number, scales = "free_x") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

For simplicity, let's remove all points following the first two consecutive short step lengths (i.e. 0 or `r min(musk_collar$steplength_m[musk_collar$steplength_m!=0], na.rm = TRUE)`) at the end of the time series.

```{r}
#| label: fix_data
#| dependson: "daterange"
#| echo: FALSE
#| warning: FALSE

xdate_707 <- as.Date("2008-01-10")
xdate_709 <- as.Date("2008-03-22")
xdate_710 <- as.Date("2008-04-10")

musk_collar_fix <- musk_collar %>%
  filter(Id_Number!=707|Date<xdate_707,
         Id_Number!=709|Date<xdate_709,
         Id_Number!=710|Date<xdate_710) %>%
  ungroup() %>%
  mutate(rowid = 1:n())
daterange_fix <- musk_collar_fix %>% 
  sf::st_drop_geometry() %>% 
  group_by(Id_Number) %>% 
  summarize(NumDays_fix = as.numeric(difftime(max(Date), min(Date), 
                                            units = "days")))
daterange %>%
  left_join(daterange_fix) %>%
  filter(Id_Number%in%c(707,709,710)) %>%
  mutate(DaysRemoved = NumDays - NumDays_fix) %>%
  select(Id_Number, DaysRemoved) %>%
  knitr::kable()

```

## Data Filtering

Now that we have removed data that we know contain errors, we can start to look for less obvious sources of errors. The SDLfilter package has two functions, *dupfilter* and *ddfilter,* that remove data duplicates and unlikely points, respectively.

```{r}
#| label: sdl_musk
#| dependson: "fix_data"
#| echo: FALSE
#| warning: FALSE

### rename columns in dataframe for SDLfilter functions
sdl_musk <- musk_collar_fix %>%
  rename(id = Id_Number,
         lat = Latitude,
         lon = Longitude,
         DateTime = datetime) %>%
  sf::st_drop_geometry() %>%
  ## the fundtions also require a column for number of satellite fixes,
  ## but we don't have those data so we'll just fill in a column with 
  ## the minimum required value of 4.
  mutate(qi = 5) %>%
  as.data.frame()
```

First let's use *dupfilter* to remove duplicate data points with the same date and time.

```{r}
#| label: sdl_dup
#| dependson: "sdl_musk"
#| echo: TRUE
#| warning: FALSE

sdl_dup <- SDLfilter::dupfilter(sdl_musk, 
                                step.time = 7,
                                conditional = TRUE)
```

Next we'll use *ddfilter* to filter points based on speed and turning angles between consecutive points. To use this function, we need to estimate the maximum reasonable speed between two consecutive locations. The *vmax* function maps a gamma distribution to the observed speeds and estimates the maximum speed as the value with a cumulative probability of 0.999.

```{r}
#| label: vmax
#| dependson: "sdl_dup"
#| echo: TRUE
#| warning: FALSE

vmax <- SDLfilter::vmax(sdl_dup, prob = 0.999)
```
We will also estimate the maximum one-way linear speed of a loop trip. A loop trip is defined as a spatial departure and return involving a series of three or more consecutive points. The *vmaxlp* function also uses a gamma distribution to estimate this speed:

```{r}
#| label: vmaxlp
#| dependson: "vmax"
#| echo: TRUE
#| warning: FALSE

vmaxlp <- SDLfilter::vmaxlp(sdl_dup, prob = 0.999)
```

Now we can filter our data using *ddfilter*. Locations are removed if one of following criteria are met: 1) speed from previous and to subsequent location both exceed **vmax**; 2) inner angle is less than a given threshold and speed from a previous or to a subsequent location exceeds **vmaxlp**. We will specify a turning angle of 14 degrees, as outlined in BjÃ¸rneraas et al. 2010.

```{r}
#| label: sdl_dd1
#| dependson: "vmaxlp"
#| echo: TRUE
#| warning: FALSE

sdl_dd1 <- SDLfilter::ddfilter(sdl_dup, qi = 5, 
                              vmax = vmax, vmaxlp = vmaxlp,
                              ia = 14)

```

```{r, fig.height=6, fig.width=8}
#| dependson: "sdl_dd1"
#| echo: FALSE
#| warning: FALSE
sdl_dup %>%
  ## identify points that were identified as duplicates
  mutate(
    flag = ifelse(rowid %in% sdl_dd1$rowid,"retained","removed"),
    steplength_m = steplength_m + 1
    ) %>%
  ggplot(aes(x = DateTime, y = steplength_m)) +
  geom_point(aes(colour = flag, size = flag)) +
  scale_y_log10() +
  facet_wrap(~id, scales = "free_x") +
  scale_size_manual(values = c(1,0.5)) +
  scale_colour_manual(values = c("red", "grey")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

The filtered points appear non-randomly across the time series, which  suggests that our speed thresholds were too small. Let's increase **vmax** and **vmaxlp** to more conservative values of 2 km/h and 1 km/h, respectively.

```{r}
#| label: sdl_dd2
#| dependson: "sdl_dup"
#| echo: TRUE
#| warning: FALSE

sdl_dd2 <- SDLfilter::ddfilter(sdl_dup, qi = 5, 
                              vmax = 2, vmaxlp = 1,
                              ia = 14)

```

```{r, fig.height=6, fig.width=8}
#| dependson: "sdl_dd2"
#| echo: FALSE
#| warning: FALSE
sdl_dup %>%
  ## identify points that were identified as duplicates
  mutate(
    flag = ifelse(rowid %in% sdl_dd2$rowid,"retained","removed"),
    steplength_m = steplength_m + 1
    ) %>%
  ggplot(aes(x = DateTime, y = steplength_m)) +
  geom_point(aes(colour = flag, size = flag)) +
  scale_y_log10() +
  facet_wrap(~id, scales = "free_x") +
  scale_size_manual(values = c(1,0.5)) +
  scale_colour_manual(values = c("red", "grey")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```
That seems to do a better job of only removing the most extreme points. 

## Post-deployment Bias

Lastly, we will remove any points collected within 24 hours of collar deployments to remove the immediate effects of capture and handling.

```{r}
#| label: musk_collar_filt
#| dependson: "sdl_dd2"
#| echo: FALSE
#| warning: FALSE

deploy <- read_csv(here("data/raw/muskox_data/musk_collar_deployments.csv"), show_col_types = FALSE) %>%
  mutate(datetime = as.POSIXct(str_c(Date, Time, sep = " ")),
         ##create cutoff time 24 hours after deployment
         date_filter = datetime + 24*60*60,
         Id_Number = as.factor(Id_Number))

deploy %>%
  select(Id_Number, datetime) %>%
  rename(collar_deployed = datetime) %>%
  knitr::kable()

musk_collar_filt <- musk_collar_fix %>%
  right_join(sdl_dd2 %>% select(rowid, pTime, pDist, pSpeed),
            by = "rowid") %>%
  left_join(deploy %>%  select(Id_Number, date_filter), 
            by = c("Id_Number")) %>%
  ## filter out observations within the 24 hour window following deployment
  filter(datetime > date_filter) %>%
  select(Id_Number, PTT, Latitude, Longitude, datetime, year, month,
         pTime, pDist, pSpeed, geometry)

saveRDS(musk_collar_filt, here("data/processed/musk_collar_filt.rds"))

### convert file to csv for MoveBank  
musk_collar_filt %>%
  sf::st_drop_geometry() %>%
  write_csv(here("data/processed/musk_collar_filt.csv"))
```


